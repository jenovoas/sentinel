#!/bin/bash
# Script para configurar Ollama keep_alive PERMANENTE
# Mantiene modelo en RAM indefinidamente (hasta reiniciar Ollama)

echo "ğŸ”§ Configurando Ollama keep_alive PERMANENTE..."
echo ""

# Modelo a mantener en RAM
MODEL="llama3.2:1b"

echo "ğŸ“‹ Opciones de keep_alive:"
echo "   -1  = PERMANENTE (nunca descarga, hasta reiniciar Ollama)"
echo "   0   = Descarga inmediatamente despuÃ©s de responder"
echo "   5m  = Mantiene 5 minutos"
echo "   1h  = Mantiene 1 hora"
echo "   24h = Mantiene 24 horas"
echo ""
echo "âœ… Usando: keep_alive = -1 (PERMANENTE)"
echo ""

# Configurar keep_alive = -1 (permanente)
echo "ğŸš€ Enviando configuraciÃ³n a Ollama..."
RESPONSE=$(curl -s http://localhost:11434/api/generate -d "{
  \"model\": \"$MODEL\",
  \"prompt\": \"Sistema iniciado. Modelo cargado en memoria.\",
  \"keep_alive\": -1,
  \"stream\": false
}")

if [ $? -eq 0 ]; then
    echo "âœ… Modelo $MODEL configurado con keep_alive = -1 (PERMANENTE)"
    echo ""
    echo "ğŸ“Š Esto significa:"
    echo "   âœ“ El modelo permanecerÃ¡ en RAM indefinidamente"
    echo "   âœ“ NO se descargarÃ¡ entre requests"
    echo "   âœ“ Latencias consistentes garantizadas"
    echo "   âœ“ Solo se descarga si reinicias Ollama"
    echo ""
    echo "ğŸ’¾ Uso de RAM:"
    echo "   Modelo llama3.2:1b: ~1.3 GB VRAM"
    echo "   GTX 1050 disponible: 3 GB VRAM"
    echo "   Espacio restante: ~1.7 GB âœ…"
    echo ""
    echo "ğŸ” Verificando modelos cargados..."
    curl -s http://localhost:11434/api/tags | python3 -m json.tool | grep -A 10 "name"
else
    echo "âŒ Error al configurar keep_alive"
    echo ""
    echo "ğŸ”§ Troubleshooting:"
    echo "   1. Verifica que Ollama estÃ© corriendo:"
    echo "      systemctl status ollama"
    echo ""
    echo "   2. Si no estÃ¡ corriendo, inÃ­cialo:"
    echo "      systemctl start ollama"
    echo ""
    echo "   3. Verifica que el modelo estÃ© descargado:"
    echo "      ollama list"
    exit 1
fi

echo ""
echo "âœ… CONFIGURACIÃ“N COMPLETA"
echo ""
echo "ğŸš€ Ahora puedes ejecutar benchmarks con latencia consistente:"
echo "   cd backend && python sentinel_global_benchmark.py"
echo ""
echo "ğŸ“ Nota: El modelo permanecerÃ¡ en RAM hasta que:"
echo "   - Reinicies Ollama (systemctl restart ollama)"
echo "   - Reinicies el sistema"
echo "   - Cambies keep_alive manualmente"
