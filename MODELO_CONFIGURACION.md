# üéØ Configuraci√≥n de Modelos - Sentinel

## ‚úÖ Modelo en Producci√≥n

**Modelo Activo**: `llama3.2:1b`

### Especificaciones
- **Tama√±o**: 1.3 GB
- **Par√°metros**: 1B
- **VRAM**: Cabe perfectamente en GTX 1050 (3GB)

### M√©tricas Validadas
- **TTFB Promedio**: 10.4s
- **TTFB M√≠nimo**: 5.3s (modelo en RAM)
- **TTFB M√°ximo**: 16.7s
- **Estabilidad**: ‚úÖ Buena (varianza moderada)

### Ventajas
- ‚úÖ **2.7x m√°s r√°pido** que phi3:mini
- ‚úÖ **63% menos latencia**
- ‚úÖ Cabe mejor en 3GB VRAM
- ‚úÖ Menos swapping CPU/GPU
- ‚úÖ Respuestas consistentes

---

## üß™ Modelos en Testing

### phi3:mini
**Estado**: Testing / Desarrollo

**Especificaciones**:
- Tama√±o: 2.2 GB
- Par√°metros: 2.7B

**M√©tricas**:
- TTFB Promedio: 28.1s
- TTFB M√≠nimo: 6.3s
- TTFB M√°ximo: 49.4s

**Raz√≥n Testing**:
- ‚ö†Ô∏è 2.7x m√°s lento que llama3.2:1b
- ‚ö†Ô∏è Alta varianza (6s - 49s)
- ‚ö†Ô∏è Requiere optimizaci√≥n adicional

**Plan de Mejora**:
1. Probar versi√≥n quantizada (phi3:mini-q4_K_M)
2. Optimizar configuraci√≥n Ollama
3. Evaluar con mejor GPU (futuro)

---

## üìä Benchmark Comparativo

| Modelo | TTFB Avg | TTFB Min | TTFB Max | Tama√±o | Estado |
|--------|----------|----------|----------|--------|--------|
| **llama3.2:1b** | **10.4s** | **5.3s** | **16.7s** | 1.3GB | ‚úÖ **PRODUCCI√ìN** |
| phi3:mini | 28.1s | 6.3s | 49.4s | 2.2GB | üß™ Testing |

**Mejora**: llama3.2:1b es **2.7x m√°s r√°pido**

---

## üîß Configuraci√≥n Actual

### Sentinel Fluido
```python
# backend/app/services/sentinel_fluido.py
model: str = "llama3.2:1b"  # Modelo por defecto
```

### Keep Alive
```bash
# Mantener modelo en memoria
bash scripts/ollama_keep_alive.sh llama3.2:1b
```

### Verificar Estado
```bash
ollama list
curl -s http://localhost:11434/api/ps
```

---

## üöÄ Uso

### C√≥digo
```python
from app.services.sentinel_fluido import sentinel_fluido

# Usa llama3.2:1b por defecto
async for chunk, ttfb in sentinel_fluido.responder("user_id", "mensaje"):
    print(chunk, end='', flush=True)

# O especificar modelo manualmente
sentinel_custom = SentinelFluido(model="phi3:mini")  # Testing
```

### Tests
```bash
# Test con modelo por defecto (llama3.2:1b)
cd backend
python test_fluido.py

# Benchmark comparativo
python benchmark_phi_vs_llama.py
```

---

## üìù Pr√≥ximos Pasos

### Corto Plazo (Esta Semana)
1. ‚úÖ Configurar llama3.2:1b como default
2. ‚úÖ Documentar resultados
3. ‚è≥ Probar en casos de uso reales
4. ‚è≥ Validar calidad de respuestas

### Mediano Plazo (1-2 Semanas)
1. Optimizar phi3:mini (quantizaci√≥n)
2. Probar otros modelos peque√±os
3. Documentar trade-offs calidad vs velocidad

### Largo Plazo (1-3 Meses)
1. Upgrade GPU (RTX 3060 12GB)
2. Migrar a vLLM
3. Implementar SPIRe + MTAD
4. Target: TTFB <200ms

---

## üéØ Criterios de Cambio

### Cambiar a otro modelo si:
- Nuevo modelo >30% m√°s r√°pido
- Calidad de respuestas significativamente mejor
- Cabe mejor en hardware actual

### Volver a phi3:mini si:
- Calidad de llama3.2:1b insuficiente
- Se optimiza phi3 a <10s TTFB
- Se hace upgrade GPU

---

**√öltima actualizaci√≥n**: 19 Diciembre 2024  
**Modelo activo**: llama3.2:1b  
**Pr√≥xima revisi√≥n**: Despu√©s de validar en producci√≥n
