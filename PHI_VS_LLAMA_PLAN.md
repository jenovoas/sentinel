# ğŸ”¬ ComparaciÃ³n phi3 vs llama3 - Plan de Pruebas

## ğŸ¯ Objetivo

Comparar latencias de diferentes modelos pequeÃ±os para encontrar el mÃ¡s rÃ¡pido en GTX 1050 (3GB VRAM).

## ğŸ“‹ Modelos a Probar

| Modelo | TamaÃ±o | ParÃ¡metros | Estado |
|--------|--------|------------|--------|
| **phi3:mini** | 2.2 GB | 2.7B | âœ… Instalado |
| **llama3.2:1b** | 1.3 GB | 1B | ğŸ”„ Descargando |

## ğŸ§ª MetodologÃ­a

### Test por Modelo
- 5 requests con mensajes variados
- Medir TTFB (Time To First Byte)
- Calcular: promedio, mediana, mÃ­n, mÃ¡x

### MÃ©tricas Clave
- **TTFB Promedio**: Latencia tÃ­pica
- **TTFB MÃ­nimo**: Mejor caso (modelo en RAM)
- **TTFB MÃ¡ximo**: Peor caso (carga desde disco)
- **Estabilidad**: Varianza entre requests

## ğŸ“Š HipÃ³tesis

### llama3.2:1b (1B params, 1.3GB)
**Ventajas**:
- âœ… MÃ¡s pequeÃ±o (cabe mejor en 3GB VRAM)
- âœ… Menos parÃ¡metros = mÃ¡s rÃ¡pido
- âœ… Menos swapping CPU/GPU

**Desventajas**:
- âš ï¸ Menor calidad de respuestas
- âš ï¸ Menos conocimiento

**TTFB Esperado**: 1-2s (mejor que phi3)

### phi3:mini (2.7B params, 2.2GB)
**Ventajas**:
- âœ… Mejor calidad de respuestas
- âœ… MÃ¡s conocimiento
- âœ… Ya probado (baseline conocido)

**Desventajas**:
- âš ï¸ MÃ¡s grande (2.2GB)
- âš ï¸ MÃ¡s parÃ¡metros = mÃ¡s lento
- âš ï¸ MÃ¡s swapping en 3GB VRAM

**TTFB Esperado**: 3-5s (baseline actual con keep_alive)

## ğŸ¯ Criterios de DecisiÃ³n

### Si llama3.2:1b es >30% mÃ¡s rÃ¡pido
â†’ **Usar llama3.2:1b** (velocidad > calidad)

### Si diferencia <30%
â†’ **Usar phi3:mini** (mejor calidad)

### Si ambos >5s TTFB
â†’ **Considerar upgrade GPU** o modelos mÃ¡s pequeÃ±os

## ğŸ“ EjecuciÃ³n

```bash
# 1. Esperar descarga llama3.2:1b
ollama list

# 2. Configurar keep_alive para ambos
bash scripts/ollama_keep_alive.sh phi3:mini
bash scripts/ollama_keep_alive.sh llama3.2:1b

# 3. Ejecutar benchmark comparativo
cd backend
python benchmark_phi_vs_llama.py
```

## ğŸ“Š Resultados Esperados

### Escenario Optimista
```
llama3.2:1b: 1-2s TTFB
phi3:mini: 3-5s TTFB
Ganador: llama3.2:1b (50% mÃ¡s rÃ¡pido)
```

### Escenario Realista
```
llama3.2:1b: 2-3s TTFB
phi3:mini: 4-6s TTFB
Ganador: llama3.2:1b (40% mÃ¡s rÃ¡pido)
```

### Escenario Pesimista
```
llama3.2:1b: 3-4s TTFB
phi3:mini: 5-7s TTFB
Ganador: llama3.2:1b (30% mÃ¡s rÃ¡pido)
```

## ğŸš€ PrÃ³ximos Pasos

1. âœ… Descargar llama3.2:1b
2. â³ Ejecutar benchmark comparativo
3. â³ Documentar resultados
4. â³ Elegir modelo ganador
5. â³ Configurar Sentinel con modelo Ã³ptimo

---

**Estado**: ğŸ”„ Descargando llama3.2:1b...  
**PrÃ³xima acciÃ³n**: Ejecutar `benchmark_phi_vs_llama.py`
