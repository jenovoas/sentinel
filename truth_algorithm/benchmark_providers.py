#!/usr/bin/env python3
"""
Truth Algorithm - Benchmark Completo de Providers
==================================================

Compara velocidad y calidad de MOCK, DuckDuckGo y Perplexity.

Powered by Google ‚ù§Ô∏è & Perplexity üíú
"""

import time
import os
from certification_generator import CertificationGenerator
from source_search import SearchProvider


def benchmark_all_providers():
    """Benchmark completo de todos los providers"""
    
    print("="*70)
    print("üöÄ TRUTH ALGORITHM - BENCHMARK COMPLETO")
    print("="*70)
    print()
    
    # Claims de prueba
    test_claims = [
        "Python programming language was created by Guido van Rossum in 1991",
        "The Earth orbits around the Sun",
        "Water boils at 100 degrees Celsius at sea level"
    ]
    
    # Providers a probar
    providers = [
        (SearchProvider.MOCK, "üé≠ MOCK"),
        (SearchProvider.DUCKDUCKGO, "ü¶Ü DuckDuckGo"),
        (SearchProvider.PERPLEXITY, "üíú Perplexity"),
    ]
    
    results = {}
    
    for provider, name in providers:
        print(f"\n{'='*70}")
        print(f"Testing: {name}")
        print(f"{'='*70}\n")
        
        provider_results = []
        
        for i, claim in enumerate(test_claims, 1):
            print(f"Claim {i}/3: {claim[:50]}...")
            
            try:
                generator = CertificationGenerator(provider=provider)
                
                start = time.time()
                certificate = generator.certify(claim)
                elapsed = time.time() - start
                
                provider_results.append({
                    'claim': claim,
                    'score': certificate.truth_score,
                    'sources': certificate.sources_total,
                    'time': elapsed * 1000,  # ms
                    'confidence': certificate.confidence_level
                })
                
                print(f"  ‚úÖ Score: {certificate.truth_score:.3f}")
                print(f"  üìä Fuentes: {certificate.sources_total}")
                print(f"  ‚è±Ô∏è  Tiempo: {elapsed*1000:.2f}ms")
                print()
                
            except Exception as e:
                print(f"  ‚ùå Error: {e}")
                print()
        
        results[name] = provider_results
    
    # Resumen comparativo
    print("\n" + "="*70)
    print("üìä RESUMEN COMPARATIVO")
    print("="*70)
    print()
    
    # Tabla de resultados
    print(f"{'Provider':<15} | {'Avg Score':<10} | {'Avg Sources':<12} | {'Avg Time':<10} | {'Status':<10}")
    print("-" * 70)
    
    for provider_name, provider_results in results.items():
        if provider_results:
            avg_score = sum(r['score'] for r in provider_results) / len(provider_results)
            avg_sources = sum(r['sources'] for r in provider_results) / len(provider_results)
            avg_time = sum(r['time'] for r in provider_results) / len(provider_results)
            
            status = "‚úÖ OK" if avg_score > 0 else "‚ö†Ô∏è  Low"
            
            print(f"{provider_name:<15} | {avg_score:<10.3f} | {avg_sources:<12.1f} | {avg_time:<10.2f} | {status:<10}")
    
    print()
    print("="*70)
    print("üèÜ GANADORES POR CATEGOR√çA")
    print("="*70)
    print()
    
    # Encontrar el mejor en cada categor√≠a
    all_results = []
    for provider_name, provider_results in results.items():
        if provider_results:
            avg_score = sum(r['score'] for r in provider_results) / len(provider_results)
            avg_time = sum(r['time'] for r in provider_results) / len(provider_results)
            avg_sources = sum(r['sources'] for r in provider_results) / len(provider_results)
            all_results.append((provider_name, avg_score, avg_time, avg_sources))
    
    if all_results:
        # Mejor score
        best_score = max(all_results, key=lambda x: x[1])
        print(f"üéØ Mejor Truth Score: {best_score[0]} ({best_score[1]:.3f})")
        
        # M√°s r√°pido
        fastest = min(all_results, key=lambda x: x[2])
        print(f"‚ö° M√°s R√°pido: {fastest[0]} ({fastest[2]:.2f}ms)")
        
        # M√°s fuentes
        most_sources = max(all_results, key=lambda x: x[3])
        print(f"üìö M√°s Fuentes: {most_sources[0]} ({most_sources[3]:.1f} promedio)")
    
    print()
    print("="*70)
    print("üí° RECOMENDACIONES")
    print("="*70)
    print()
    print("üé≠ MOCK: Testing y desarrollo (instant√°neo)")
    print("ü¶Ü DuckDuckGo: Producci√≥n gratis, claims generales (r√°pido)")
    print("üíú Perplexity: Claims t√©cnicos espec√≠ficos, m√°xima calidad (lento pero preciso)")
    print()
    print("="*70)


if __name__ == '__main__':
    # Verificar Perplexity API key
    if not os.getenv('PERPLEXITY_API_KEY'):
        print("‚ö†Ô∏è  PERPLEXITY_API_KEY no configurada")
        print("   El benchmark de Perplexity usar√° fallback a MOCK")
        print()
    
    benchmark_all_providers()
