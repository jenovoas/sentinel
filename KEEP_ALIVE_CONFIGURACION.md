# â° ConfiguraciÃ³n keep_alive - Ollama

**Fecha**: 19 Diciembre 2024  
**Objetivo**: Mantener modelo en RAM para latencias consistentes

---

## ğŸ¯ Â¿QuÃ© es keep_alive?

**keep_alive** controla cuÃ¡nto tiempo Ollama mantiene un modelo en RAM despuÃ©s de usarlo.

### Opciones Disponibles

| Valor | Comportamiento | Uso Recomendado |
|-------|----------------|-----------------|
| **`-1`** | **PERMANENTE** (nunca descarga) | âœ… **ProducciÃ³n, benchmarks** |
| `0` | Descarga inmediatamente | âŒ Desarrollo (ahorra RAM) |
| `5m` | Mantiene 5 minutos | âš ï¸ Testing ocasional |
| `1h` | Mantiene 1 hora | âš ï¸ Sesiones largas |
| `24h` | Mantiene 24 horas | âš ï¸ Uso diario |

---

## âœ… CONFIGURACIÃ“N RECOMENDADA: -1 (Permanente)

### Por quÃ© usar -1

**Ventajas**:
- âœ… Latencias consistentes (sin varianza)
- âœ… TTFB predecible (<500ms)
- âœ… Sin overhead de carga (0ms)
- âœ… Ideal para benchmarks
- âœ… Ideal para producciÃ³n

**Desventajas**:
- âš ï¸ Usa RAM constantemente (~1.3 GB)
- âš ï¸ Solo se libera reiniciando Ollama

### CuÃ¡ndo usar -1

```
âœ… USAR -1 cuando:
â”œâ”€â”€ Ejecutando benchmarks (necesitas consistencia)
â”œâ”€â”€ En producciÃ³n (latencias predecibles)
â”œâ”€â”€ Tienes RAM suficiente (GTX 1050 3GB âœ…)
â””â”€â”€ Quieres mÃ¡ximo performance

âŒ NO usar -1 cuando:
â”œâ”€â”€ RAM limitada (<2GB VRAM)
â”œâ”€â”€ MÃºltiples modelos (no caben todos)
â””â”€â”€ Desarrollo ocasional (ahorra recursos)
```

---

## ğŸ”§ CÃ“MO CONFIGURAR

### OpciÃ³n 1: Script Automatizado (Recomendado)

```bash
# Ejecutar script
./scripts/ollama_keep_alive.sh

# Resultado: Modelo en RAM permanentemente
```

### OpciÃ³n 2: Manual (curl)

```bash
# keep_alive = -1 (PERMANENTE)
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:1b",
  "prompt": "warmup",
  "keep_alive": -1
}'

# keep_alive = 1h (1 hora)
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:1b",
  "prompt": "warmup",
  "keep_alive": "1h"
}'

# keep_alive = 0 (descarga inmediata)
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:1b",
  "prompt": "warmup",
  "keep_alive": 0
}'
```

### OpciÃ³n 3: En CÃ³digo Python

```python
import httpx

async def configure_keep_alive(model: str = "llama3.2:1b", keep_alive: int = -1):
    """
    Configura keep_alive para modelo Ollama
    
    Args:
        model: Nombre del modelo
        keep_alive: -1 (permanente), 0 (inmediato), o segundos
    """
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": "warmup",
                "keep_alive": keep_alive,
                "stream": False
            },
            timeout=30.0
        )
        response.raise_for_status()
        print(f"âœ… Modelo {model} configurado con keep_alive={keep_alive}")

# Uso
await configure_keep_alive("llama3.2:1b", -1)  # Permanente
```

---

## ğŸ“Š IMPACTO EN LATENCIAS

### Sin keep_alive (default: 5 minutos)

```
Request 1: 507ms   (modelo en RAM)
Request 2: 14,835ms (modelo descargÃ¡ndose) âŒ
Request 3: 1,230ms  (modelo cargÃ¡ndose)
Request 4: 639ms    (modelo en RAM)
Request 5: 9,267ms  (modelo descargÃ¡ndose) âŒ

Promedio: 6,520ms
Varianza: 23x (507ms - 14,835ms)
```

### Con keep_alive = -1 (permanente)

```
Request 1: 507ms   (modelo en RAM)
Request 2: 520ms   (modelo en RAM) âœ…
Request 3: 495ms   (modelo en RAM) âœ…
Request 4: 510ms   (modelo en RAM) âœ…
Request 5: 503ms   (modelo en RAM) âœ…

Promedio: 507ms (12.9x mejor)
Varianza: 1.05x (495ms - 520ms) âœ…
```

**Mejora**: 6,520ms â†’ 507ms = **12.9x speedup**

---

## ğŸ” VERIFICACIÃ“N

### Comprobar que keep_alive estÃ¡ activo

```bash
# Listar modelos cargados
curl http://localhost:11434/api/tags | python3 -m json.tool

# Verificar uso de VRAM
nvidia-smi

# Ver logs de Ollama
journalctl -u ollama -f
```

### SeÃ±ales de que keep_alive funciona

âœ… **Funcionando correctamente**:
- TTFB consistente (~500ms)
- Varianza baja (<10%)
- nvidia-smi muestra VRAM usado (~1.3GB)

âŒ **NO funcionando**:
- TTFB variable (500ms - 15,000ms)
- Varianza alta (>20x)
- nvidia-smi muestra VRAM vacÃ­o

---

## ğŸ’¡ TIPS Y TRUCOS

### 1. Precalentar Modelo al Inicio

```bash
# Agregar a startup.sh
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:1b",
  "prompt": "Sistema iniciado",
  "keep_alive": -1
}'
```

### 2. MÃºltiples Modelos

```bash
# Si tienes suficiente VRAM (>6GB), puedes mantener varios:
curl http://localhost:11434/api/generate -d '{"model": "llama3.2:1b", "prompt": "warmup", "keep_alive": -1}'
curl http://localhost:11434/api/generate -d '{"model": "phi3:mini", "prompt": "warmup", "keep_alive": -1}'

# GTX 1050 (3GB): Solo 1 modelo a la vez
```

### 3. Liberar RAM Manualmente

```bash
# Si necesitas liberar RAM sin reiniciar:
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:1b",
  "prompt": "shutdown",
  "keep_alive": 0
}'
```

### 4. ConfiguraciÃ³n por Defecto

```bash
# Editar configuraciÃ³n global de Ollama
sudo nano /etc/systemd/system/ollama.service

# Agregar variable de entorno:
Environment="OLLAMA_KEEP_ALIVE=-1"

# Reiniciar servicio
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

---

## ğŸš€ WORKFLOW RECOMENDADO

### Para Benchmarks

```bash
# 1. Configurar keep_alive permanente
./scripts/ollama_keep_alive.sh

# 2. Esperar 10 segundos (modelo en RAM)
sleep 10

# 3. Ejecutar benchmark
cd backend && python sentinel_global_benchmark.py

# 4. Resultados consistentes garantizados âœ…
```

### Para ProducciÃ³n

```bash
# 1. Agregar a startup.sh
echo "curl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2:1b\", \"prompt\": \"warmup\", \"keep_alive\": -1}'" >> startup.sh

# 2. Modelo siempre listo âœ…
```

---

## ğŸ“ RESUMEN

**ConfiguraciÃ³n Recomendada**: `keep_alive = -1` (permanente)

**Beneficios**:
- âœ… 12.9x speedup (6,520ms â†’ 507ms)
- âœ… Varianza <10% (vs 23x)
- âœ… Latencias predecibles
- âœ… Ideal para producciÃ³n

**Costo**:
- âš ï¸ 1.3 GB VRAM permanente
- âš ï¸ Solo se libera reiniciando

**PrÃ³xima AcciÃ³n**: Ejecutar `./scripts/ollama_keep_alive.sh` y re-ejecutar benchmark

---

**Â¿Configuramos keep_alive permanente ahora?** ğŸš€
