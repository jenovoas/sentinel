# ü§ñ Integraci√≥n de IA Local (Ollama) - Resumen de Implementaci√≥n

**Fecha**: 14 de Diciembre, 2025  
**Estado**: ‚úÖ En progreso  
**Modelo**: phi3:mini + llama3.2:1b

---

## ‚úÖ Cambios Implementados

### 1. Docker Compose
- ‚úÖ Agregado servicio `ollama` (puerto 11434)
- ‚úÖ Agregado servicio `ollama-init` para descargar modelos
- ‚úÖ Creado volumen `ollama_data`
- ‚úÖ Configurado healthcheck para Ollama

### 2. Variables de Entorno
- ‚úÖ Actualizado `.env.example` con configuraci√≥n de Ollama:
  - `OLLAMA_URL=http://ollama:11434`
  - `OLLAMA_MODEL=phi3:mini`
  - `AI_ENABLED=true`
  - `OLLAMA_TIMEOUT=8`
  - `OLLAMA_NUM_PREDICT=100`
  - `OLLAMA_TEMPERATURE=0.3`

### 3. Backend API
- ‚úÖ Creado `/backend/app/routers/ai.py` con 3 endpoints:
  - `POST /api/v1/ai/query` - Consultar IA
  - `GET /api/v1/ai/health` - Estado del servicio
  - `POST /api/v1/ai/analyze-anomaly` - Analizar anomal√≠as
- ‚úÖ Registrado router en `main.py`

---

## üîÑ En Progreso

### Descarga de Ollama
- ‚è≥ Descargando imagen de Ollama (2.1 GB)
- ‚è≥ Esperando inicio del servicio

---

## üìã Pr√≥ximos Pasos

### 1. Integrar IA en AnomalyDetector
Modificar `backend/app/services/anomaly_detector.py` para enriquecer anomal√≠as con explicaciones de IA.

### 2. Mejorar Watchdog de Seguridad
Actualizar `host-metrics/audit-watchdog.sh` para an√°lisis inteligente de eventos.

### 3. Actualizar Backend Config
Agregar configuraci√≥n de Ollama en `backend/app/config.py`.

### 4. Verificar Integraci√≥n
- Probar endpoint `/api/v1/ai/health`
- Probar consulta de IA
- Verificar enriquecimiento de anomal√≠as

---

## üß™ Tests de Verificaci√≥n

### 1. Verificar Servicio Ollama
```bash
# Esperar a que Ollama est√© listo
docker-compose logs -f ollama

# Verificar que responde
curl http://localhost:11434/api/tags

# Deber√≠a retornar lista de modelos
```

### 2. Descargar Modelos
```bash
# Ejecutar ollama-init
docker-compose up ollama-init

# Ver progreso
docker-compose logs -f ollama-init

# Verificar modelos descargados
curl http://localhost:11434/api/tags | jq '.models[].name'
```

### 3. Probar Endpoint de IA
```bash
# Reiniciar backend con nuevos cambios
docker-compose restart backend

# Probar health check
curl http://localhost:8000/api/v1/ai/health | jq

# Probar consulta
curl -X POST http://localhost:8000/api/v1/ai/query \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explica qu√© es una anomal√≠a de CPU en 1 l√≠nea",
    "max_tokens": 50,
    "temperature": 0.3
  }' | jq
```

### 4. Probar An√°lisis de Anomal√≠as
```bash
# Analizar una anomal√≠a
curl -X POST "http://localhost:8000/api/v1/ai/analyze-anomaly?title=CPU%20Spike&description=CPU%20at%2085%25&metric_value=85&threshold_value=80" | jq
```

---

## üìä Recursos del Sistema

### Ollama
- **RAM**: 2-4 GB (modelo phi3:mini cargado)
- **Disco**: ~2 GB por modelo
- **CPU**: Moderado sin GPU
- **Latencia**: 1-3 segundos por query

### Modelos Descargados
- `phi3:mini` (1.3B params) - R√°pido, ligero
- `llama3.2:1b` (1B params) - Muy r√°pido

---

## üîß Configuraci√≥n

### Deshabilitar IA Temporalmente
```bash
# En .env
AI_ENABLED=false

# Reiniciar backend
docker-compose restart backend
```

### Cambiar Modelo
```bash
# En .env
OLLAMA_MODEL=llama3.2:1b

# Reiniciar backend
docker-compose restart backend
```

---

## üìù Archivos Modificados

1. `docker-compose.yml` - Agregado Ollama y ollama-init
2. `.env.example` - Agregada configuraci√≥n de IA
3. `backend/app/routers/ai.py` - Nuevo router de IA
4. `backend/app/main.py` - Registrado router de IA

---

## üéØ Estado Actual

- ‚úÖ Servicio Ollama agregado a docker-compose
- ‚úÖ Variables de entorno configuradas
- ‚úÖ Endpoint de IA creado en backend
- ‚è≥ Descargando imagen de Ollama
- ‚è≥ Pendiente: Integraci√≥n con AnomalyDetector
- ‚è≥ Pendiente: Mejora de watchdog de seguridad

---

**Pr√≥xima acci√≥n**: Esperar descarga de Ollama y probar endpoints de IA
