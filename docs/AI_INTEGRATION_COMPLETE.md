# ‚úÖ Integraci√≥n de IA Local (Ollama) - COMPLETADA

**Fecha**: 14 de Diciembre, 2025  
**Estado**: üü¢ Funcionando con GPU  
**Modelo**: phi3:mini  
**GPU**: NVIDIA GeForce GTX 1050 (3GB VRAM)

---

## üéâ Resumen de Implementaci√≥n

### ‚úÖ Completado

1. **NVIDIA Container Toolkit Instalado**
   - Versi√≥n: 1.18.1-1
   - Instalado v√≠a pacman en Arch Linux
   - Docker configurado con NVIDIA runtime
   - GPU detectada correctamente

2. **Ollama con GPU**
   - Servicio corriendo en puerto 11434
   - GPU detectada: GTX 1050 (CUDA 6.1, 2.9GB VRAM)
   - Modo "low vram" activado autom√°ticamente
   - Modelo phi3:mini descargado (2.2GB)

3. **Backend API**
   - Endpoint `/api/v1/ai/query` - Consultar IA
   - Endpoint `/api/v1/ai/health` - Estado del servicio
   - Endpoint `/api/v1/ai/analyze-anomaly` - Analizar anomal√≠as
   - Router registrado en main.py

4. **Docker Compose**
   - Servicio `ollama` con GPU support
   - Servicio `ollama-init` para descargar modelos
   - Volumen `ollama_data` creado

5. **Variables de Entorno**
   - Configuraci√≥n en `.env.example`
   - `OLLAMA_URL=http://ollama:11434`
   - `OLLAMA_MODEL=phi3:mini`
   - `AI_ENABLED=true`

6. **Host Ollama Desactivado**
   - Servicio systemd detenido
   - Servicio systemd deshabilitado
   - Puerto 11434 liberado para Docker

---

## üöÄ Rendimiento

### Test de Latencia

**Consulta directa a Ollama**:
```
Prompt: "Explica en 1 l√≠nea qu√© es una anomal√≠a de CPU"
Tiempo: 9.7 segundos
Respuesta: "Una anomal√≠a de CPU se refiere a cualquier desviaci√≥n o fallo 
que afecte su funcionamiento normal, como sobrecalentamiento, problemas 
con el arreglo l√≥gico y temporal (TLB) o errores en la memoria cach√©."
```

**Nota**: Primera inferencia siempre es m√°s lenta (carga modelo en VRAM).
Inferencias subsecuentes ser√°n ~1-2 segundos.

### GPU Utilization

- **VRAM Usada**: ~2GB (modelo phi3:mini)
- **VRAM Disponible**: 2.9GB / 3GB
- **Compute Capability**: 6.1 (Pascal architecture)
- **Modo**: Low VRAM (optimizado para GPUs <20GB)

---

## üìä Servicios Activos

```bash
# Ollama
http://localhost:11434

# Backend AI Endpoints
http://localhost:8000/api/v1/ai/health
http://localhost:8000/api/v1/ai/query
http://localhost:8000/api/v1/ai/analyze-anomaly
```

---

## üß™ Comandos de Verificaci√≥n

### Verificar GPU en Docker
```bash
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

### Verificar Ollama
```bash
# Ver modelos instalados
curl http://localhost:11434/api/tags | jq '.models[].name'

# Test de inferencia
curl -X POST http://localhost:11434/api/generate \
  -d '{"model":"phi3:mini","prompt":"Hola","stream":false}' | jq -r '.response'
```

### Verificar Backend
```bash
# Health check
curl http://localhost:8000/api/v1/ai/health | jq

# Query
curl -X POST http://localhost:8000/api/v1/ai/query \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Test","max_tokens":30}' | jq
```

### Ver logs de Ollama
```bash
docker-compose logs -f ollama | grep -i "gpu\|cuda"
```

---

## üìÅ Archivos Modificados

1. `docker-compose.yml`
   - Agregado servicio `ollama` con GPU support
   - Agregado servicio `ollama-init`
   - Agregado volumen `ollama_data`

2. `.env.example`
   - Agregada secci√≥n de configuraci√≥n Ollama

3. `backend/app/routers/ai.py`
   - Nuevo router con 3 endpoints

4. `backend/app/main.py`
   - Importado y registrado router AI

5. `/etc/docker/daemon.json`
   - Configurado NVIDIA runtime

---

## üîÑ Pr√≥ximos Pasos

### 1. Integrar IA en AnomalyDetector
Modificar `backend/app/services/anomaly_detector.py` para enriquecer anomal√≠as con explicaciones de IA.

### 2. Mejorar Watchdog de Seguridad
Actualizar `host-metrics/audit-watchdog.sh` para an√°lisis inteligente de eventos.

### 3. Descargar Modelo Adicional
```bash
docker-compose exec ollama ollama pull llama3.2:1b
```

### 4. Crear Dashboards de IA
- M√©tricas de latencia de IA
- Uso de VRAM
- Queries por minuto

---

## üêõ Troubleshooting

### Ollama no detecta GPU
```bash
# Verificar NVIDIA Container Toolkit
nvidia-ctk --version

# Verificar configuraci√≥n de Docker
cat /etc/docker/daemon.json

# Reiniciar Docker
sudo systemctl restart docker
```

### Modelo no descarga
```bash
# Descargar manualmente
docker-compose exec ollama ollama pull phi3:mini

# Ver espacio en disco
df -h
```

### Backend no conecta con Ollama
```bash
# Verificar que Ollama est√© corriendo
docker-compose ps ollama

# Verificar red
docker-compose exec backend ping ollama

# Ver logs
docker-compose logs backend | grep -i ollama
```

---

## üìà Modelos Recomendados para GTX 1050 (3GB)

| Modelo | Tama√±o | VRAM | Velocidad | Calidad |
|--------|--------|------|-----------|---------|
| ‚úÖ phi3:mini | 1.3B | ~2GB | R√°pido | Buena |
| ‚úÖ llama3.2:1b | 1B | ~1.5GB | Muy r√°pido | Aceptable |
| ‚ö†Ô∏è llama3.2:3b | 3B | ~2.5GB | Moderado | Muy buena |
| ‚ùå llama3:8b | 8B | ~5GB | - | No cabe |

---

## üéØ Estado Final

- ‚úÖ NVIDIA Container Toolkit instalado
- ‚úÖ Ollama corriendo con GPU (GTX 1050)
- ‚úÖ Modelo phi3:mini descargado
- ‚úÖ Backend API funcionando
- ‚úÖ Host Ollama desactivado
- ‚è≥ Pendiente: Integraci√≥n con AnomalyDetector
- ‚è≥ Pendiente: Mejora de watchdog

**Pr√≥xima acci√≥n**: Integrar IA en detector de anomal√≠as para explicaciones autom√°ticas
