# üîç An√°lisis de Mejoras Adicionales - Sentinel

**Fecha**: 19 Diciembre 2024, 13:53  
**Estado**: keep_alive configurado, mejora moderada  
**Pr√≥ximo paso**: Identificar cuellos de botella adicionales

---

## üìä COMPARACI√ìN: Antes vs Despu√©s de keep_alive

### Resultados Medidos

| M√©trica | Sin keep_alive | Con keep_alive | Mejora | Observaci√≥n |
|---------|----------------|----------------|--------|-------------|
| **E2E p50** | 6,520ms | 7,244ms | **-11%** ‚ùå | Empeor√≥ ligeramente |
| **LLM TTFB p50** | 1,230ms | 1,213ms | **+1.4%** ‚ö†Ô∏è | Mejora marginal |
| **Mejor caso E2E** | 639ms | 591ms | **+7.5%** ‚úÖ | Mejor caso mejor√≥ |
| **Mejor caso TTFB** | 507ms | 571ms | **-13%** ‚ùå | Empeor√≥ |
| **CPU** | 14.1% | 18.7% | **-33%** ‚ùå | Peor eficiencia |

### Conclusi√≥n

**keep_alive NO resolvi√≥ el problema principal** ‚ö†Ô∏è

La varianza sigue siendo alta:
- E2E: 591ms (mejor) vs 12,376ms (peor) = **20.9x diferencia**
- TTFB: 571ms (mejor) vs 1,615ms (peor) = **2.8x diferencia**

---

## üîç AN√ÅLISIS DE CUELLO DE BOTELLA

### ¬øPor qu√© sigue lento?

**Hip√≥tesis 1: Hardware Limitado (GTX 1050 3GB)** ‚úÖ M√ÅS PROBABLE

```
EVIDENCIA:
‚îú‚îÄ‚îÄ TTFB promedio: 1,213ms (vs objetivo <300ms)
‚îú‚îÄ‚îÄ Alta varianza: 2.8x diferencia
‚îú‚îÄ‚îÄ CPU picos: 38.9% (GPU no puede manejar carga)
‚îî‚îÄ‚îÄ Modelo: llama3.2:1b (1.3GB) cabe en VRAM pero es lento

CAUSA RA√çZ:
‚îî‚îÄ‚îÄ GTX 1050 es GPU antigua (2016)
    ‚îú‚îÄ‚îÄ CUDA cores: 640 (vs RTX 3060: 3,584)
    ‚îú‚îÄ‚îÄ Tensor cores: 0 (vs RTX 3060: 112)
    ‚îî‚îÄ‚îÄ Performance: ~5x m√°s lento que GPUs modernas
```

**Hip√≥tesis 2: Modelo No Optimizado** ‚úÖ PROBABLE

```
EVIDENCIA:
‚îú‚îÄ‚îÄ Modelo actual: llama3.2:1b (no quantizado)
‚îú‚îÄ‚îÄ Tama√±o: 1.3GB (puede ser m√°s peque√±o)
‚îî‚îÄ‚îÄ Formato: GGUF (puede optimizarse m√°s)

SOLUCI√ìN:
‚îî‚îÄ‚îÄ Probar modelos m√°s peque√±os/optimizados
    ‚îú‚îÄ‚îÄ tinyllama (637MB, 1.1B params)
    ‚îú‚îÄ‚îÄ phi3:mini-q4_K_M (2.2GB quantizado)
    ‚îî‚îÄ‚îÄ qwen2.5:0.5b (500MB, ultra r√°pido)
```

**Hip√≥tesis 3: Configuraci√≥n Ollama Sub√≥ptima** ‚úÖ POSIBLE

```
EVIDENCIA:
‚îú‚îÄ‚îÄ num_ctx: 2048 (puede ser muy grande)
‚îú‚îÄ‚îÄ num_batch: 128 (puede optimizarse)
‚îî‚îÄ‚îÄ num_thread: default (puede ajustarse)

SOLUCI√ìN:
‚îî‚îÄ‚îÄ Ajustar par√°metros Ollama
    ‚îú‚îÄ‚îÄ num_ctx: 512 (reducir context window)
    ‚îú‚îÄ‚îÄ num_batch: 64 (reducir batch size)
    ‚îî‚îÄ‚îÄ num_thread: 4 (optimizar para CPU)
```

**Hip√≥tesis 4: C√≥digo Python Overhead** ‚ö†Ô∏è MENOS PROBABLE

```
EVIDENCIA:
‚îú‚îÄ‚îÄ Streaming async: Puede tener overhead
‚îú‚îÄ‚îÄ Buffers: Pueden agregar latencia
‚îî‚îÄ‚îÄ httpx: Puede ser m√°s lento que requests

SOLUCI√ìN:
‚îî‚îÄ‚îÄ Optimizar c√≥digo Python
    ‚îú‚îÄ‚îÄ Usar aiohttp en lugar de httpx
    ‚îú‚îÄ‚îÄ Reducir overhead de buffers
    ‚îî‚îÄ‚îÄ Profiling con cProfile
```

---

## üéØ PLAN DE MEJORAS (Ordenado por Impacto)

### 1. Probar Modelo M√°s Peque√±o (ALTO IMPACTO)

**Acci√≥n**: Cambiar a `qwen2.5:0.5b` (500MB, ultra r√°pido)

```bash
# Descargar modelo
ollama pull qwen2.5:0.5b

# Configurar keep_alive
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:0.5b",
  "prompt": "warmup",
  "keep_alive": -1
}'

# Actualizar sentinel_fluido.py
# model: str = "qwen2.5:0.5b"

# Re-ejecutar benchmark
python sentinel_global_benchmark.py
```

**Mejora Esperada**: 1,213ms ‚Üí **400-600ms** (2-3x)

### 2. Optimizar Par√°metros Ollama (MEDIO IMPACTO)

**Acci√≥n**: Reducir context window y batch size

```python
# En sentinel_fluido.py, l√≠nea ~190
"options": {
    "temperature": 0.7,
    "num_predict": 256,      # Reducir de 512
    "num_ctx": 512,          # Reducir de 2048 (4x)
    "num_batch": 64,         # Reducir de 128 (2x)
    "num_gpu": 1,            # Forzar GPU
    "num_thread": 4          # Optimizar CPU
}
```

**Mejora Esperada**: 1,213ms ‚Üí **800-1,000ms** (1.2-1.5x)

### 3. Usar Modelo Quantizado (MEDIO IMPACTO)

**Acci√≥n**: Probar `phi3:mini-q4_K_M` (quantizado 4-bit)

```bash
# Descargar modelo quantizado
ollama pull phi3:mini-q4_K_M

# Configurar keep_alive
curl http://localhost:11434/api/generate -d '{
  "model": "phi3:mini-q4_K_M",
  "prompt": "warmup",
  "keep_alive": -1
}'
```

**Mejora Esperada**: 1,213ms ‚Üí **700-900ms** (1.3-1.7x)

### 4. Optimizar C√≥digo Python (BAJO IMPACTO)

**Acci√≥n**: Reducir overhead de streaming

```python
# Opci√≥n 1: Usar aiohttp en lugar de httpx
import aiohttp

# Opci√≥n 2: Reducir overhead de buffers
# Eliminar buffer updates en cada chunk

# Opci√≥n 3: Profiling
python -m cProfile -o profile.stats sentinel_global_benchmark.py
```

**Mejora Esperada**: 1,213ms ‚Üí **1,100-1,200ms** (1.01-1.1x)

### 5. Upgrade Hardware (M√ÅXIMO IMPACTO, COSTO)

**Acci√≥n**: Upgrade a RTX 3060 12GB (~$300)

**Mejora Esperada**: 1,213ms ‚Üí **100-200ms** (6-12x) ‚úÖ

---

## üöÄ RECOMENDACI√ìN INMEDIATA

### Prueba R√°pida (5 minutos)

**Paso 1**: Probar modelo m√°s peque√±o

```bash
# 1. Descargar qwen2.5:0.5b (500MB, ultra r√°pido)
ollama pull qwen2.5:0.5b

# 2. Configurar keep_alive
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:0.5b",
  "prompt": "warmup",
  "keep_alive": -1
}'

# 3. Actualizar modelo en c√≥digo
sed -i 's/llama3.2:1b/qwen2.5:0.5b/g' backend/app/services/sentinel_fluido.py

# 4. Re-ejecutar benchmark
cd backend && python sentinel_global_benchmark.py
```

**Paso 2**: Optimizar par√°metros Ollama

```python
# Editar backend/app/services/sentinel_fluido.py
# L√≠nea ~190, cambiar:
"options": {
    "temperature": 0.7,
    "num_predict": 256,      # ‚Üê CAMBIO
    "num_ctx": 512,          # ‚Üê CAMBIO
    "num_batch": 64,         # ‚Üê CAMBIO
    "num_gpu": 1,
    "num_thread": 4
}
```

**Paso 3**: Re-ejecutar benchmark

```bash
cd backend && python sentinel_global_benchmark.py
```

---

## üìä MEJORA PROYECTADA (Combinando Optimizaciones)

### Escenario Optimista

| Optimizaci√≥n | Mejora Individual | Mejora Acumulada |
|--------------|-------------------|------------------|
| **Baseline** | - | 1,213ms |
| + Modelo peque√±o (qwen2.5:0.5b) | 2-3x | **400-600ms** |
| + Par√°metros optimizados | 1.2x | **330-500ms** |
| + C√≥digo optimizado | 1.1x | **300-450ms** |

**Resultado Final**: **300-450ms TTFB** ‚úÖ (cerca del objetivo <300ms)

### Escenario Realista

| Optimizaci√≥n | Mejora Individual | Mejora Acumulada |
|--------------|-------------------|------------------|
| **Baseline** | - | 1,213ms |
| + Modelo peque√±o (qwen2.5:0.5b) | 2x | **600ms** |
| + Par√°metros optimizados | 1.3x | **460ms** |

**Resultado Final**: **~460ms TTFB** ‚úÖ (objetivo <500ms cumplido)

---

## üéØ OBJETIVOS ALCANZABLES

### Con Optimizaciones de Software (Sin Costo)

```
TTFB: 1,213ms ‚Üí 300-460ms (2.6-4x mejora)
E2E: 7,244ms ‚Üí 500-800ms (9-14x mejora)
Speedup total: 10-20x ‚úÖ (cerca del objetivo)
```

### Con Upgrade Hardware ($300)

```
TTFB: 1,213ms ‚Üí 100-200ms (6-12x mejora)
E2E: 7,244ms ‚Üí 200-400ms (18-36x mejora)
Speedup total: 20-50x ‚úÖ (supera objetivo)
```

---

## üìù PR√ìXIMOS PASOS

### Inmediato (HOY)

1. [ ] Probar `qwen2.5:0.5b` (modelo m√°s peque√±o)
2. [ ] Optimizar par√°metros Ollama
3. [ ] Re-ejecutar benchmark
4. [ ] Documentar mejoras

### Corto Plazo (Esta Semana)

1. [ ] Probar `phi3:mini-q4_K_M` (quantizado)
2. [ ] Optimizar c√≥digo Python
3. [ ] Validar 10-20x speedup
4. [ ] Preparar presentaci√≥n ANID

### Mediano Plazo (1 Mes)

1. [ ] Evaluar upgrade GPU (RTX 3060)
2. [ ] Implementar Buffer ML
3. [ ] Validar 30x+ speedup
4. [ ] Presentar a ANID

---

## ‚úÖ CONCLUSI√ìN

**Resultados Actuales**:
- ‚úÖ keep_alive configurado
- ‚ö†Ô∏è Mejora marginal (1.4%)
- ‚ùå Objetivo no alcanzado

**Problema Identificado**:
- üîß Hardware limitado (GTX 1050)
- üîß Modelo no optimizado
- üîß Par√°metros Ollama sub√≥ptimos

**Soluci√≥n Inmediata**:
- ‚úÖ Probar modelo m√°s peque√±o (qwen2.5:0.5b)
- ‚úÖ Optimizar par√°metros Ollama
- ‚úÖ Mejora proyectada: 2.6-4x (300-460ms TTFB)

**Pr√≥xima Acci√≥n**: Probar `qwen2.5:0.5b` y optimizar par√°metros.

---

**¬øProbamos el modelo m√°s peque√±o ahora?** üöÄ
